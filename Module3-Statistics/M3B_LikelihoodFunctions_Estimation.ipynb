{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Basic Probability and Statistics - Likelihood Functions and Estimation\n",
    "<html>\n",
    "    <summary></summary>\n",
    "         <div> <p></p> </div>\n",
    "         <div style=\"font-size: 20px; width: 800px;\"> \n",
    "              <h1>\n",
    "               <left>Intro to Basic Probability and Statistics: Likelihood Functions and Estimation.</left>\n",
    "              </h1>\n",
    "              <p><left>============================================================================</left> </p>\n",
    "<pre>Course: BIOM 421, Spring 2024\n",
    "Instructor: Brian Munsky\n",
    "Authors: Huy Vo, Ania Baetica, Brian Munsky\n",
    "Contact Info: munsky@colostate.edu\n",
    "</pre>\n",
    "         </div>\n",
    "    </p>\n",
    "\n",
    "</html>\n",
    "\n",
    "<details>\n",
    "  <summary>Copyright info</summary>\n",
    "\n",
    "```\n",
    "Copyright 2024 Brian Munsky\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "```\n",
    "<details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood functions\n",
    "\n",
    "Likelihood functions serve as fundamental tools for quantifying the plausibility of different parameter values given observed data. Likelihood functions form the backbone of various statistical methods, including maximum likelihood estimation (MLE), Bayesian inference, and hypothesis testing.\n",
    "\n",
    "Likelihood functions provide a formal framework for evaluating how well different parameter values explain the observed data. By assessing the likelihood of observing the data under different parameter settings, we can infer the most plausible values for the parameters governing the underlying statistical model. This process of parameter estimation is essential for making predictions, testing hypotheses, and understanding the underlying mechanisms driving observed phenomena.\n",
    "\n",
    "In this introduction, we will explore the concept of likelihood functions, their role in parameter estimation, and their practical applications in statistical analysis. We will delve into the principles of maximum likelihood estimation, which seeks to find the parameter values that maximize the likelihood function. Later in the course, we will also explore Bayesian inference, which incorporates prior knowledge about the parameters into the estimation process. \n",
    "\n",
    "Additionally, we will discuss the importance of likelihood-based approaches in biology and medicine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Definition of Likelihood function\n",
    "The likelihood function is a fundamental concept in statistical inference that quantifies the plausibility of different parameter values given observed data. Let $\\theta$ denote the parameter of interest in a statistical model, and let ($x_1, x_2, \\ldots, x_n$) represent a sample of $n$ random variables sampled from the population. \n",
    "\n",
    "The likelihood function, denoted by  $L(\\theta) \\equiv L(\\theta | x_1, x_2, \\ldots, x_n) = \\text{Prob}(x_1, x_2, \\ldots, x_n | \\theta ) $, is defined as the joint probability density function (PDF) or probability mass function (PMF) of the observed data when considered as a function of the parameter $\\theta$. \n",
    "\n",
    "Under the assumption of that  ($x_1, x_2, \\ldots, x_n$) are **independent and identically distributed** (i.i.d.) random variables, then the likelihood can be expressed:\n",
    "\n",
    "$L(\\theta) = \\prod_{i=1}^n \\text{Prob}(x_i|\\theta)$\n",
    "\n",
    "When dealing with products of many numbers, it is common practice to describe likelihood functions in terms of their logarthms:\n",
    "\n",
    "$\\log L(\\theta) = \\sum_{i=1}^n \\log \\text{Prob}(x_i|\\theta)$\n",
    "\n",
    "The likelihood function provides a measure of how well different parameter values explain the observed data. Intuitively, parameter values that result in a higher likelihood are considered more plausible given the observed data. Therefore, the likelihood function forms the basis for parameter estimation techniques such as maximum likelihood estimation (MLE) and Bayesian inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Example\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_n$ be a sample of $n$ independent and identically distributed random variables from a Gaussian (normal) distribution with mean $\\mu$ and variance $\\sigma^2$. The probability density function (PDF) of a single observation $X_i$ is given by:\n",
    "\n",
    "$$\n",
    "f(x_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The likelihood function for the entire sample, denoted $L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n)$, is the product of the PDFs of the individual observations, since the observations are independent. Therefore, the likelihood function is given by:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = \\prod_{i=1}^{n} f(x_i \\mid \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "To simplify calculations and avoid numerical underflow or overflow, it's common practice to work with the logarithm of the likelihood function. Taking the logarithm of both sides of the likelihood function yields:\n",
    "\n",
    "$$\n",
    "\\log L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "This logarithm of the likelihood function is often used in practice for parameter estimation using maximum likelihood estimation (MLE) or Bayesian inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the MLE by plotting the likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of maximum likelihood estimation for gaussian distribution\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Generate some random data\n",
    "np.random.seed(0)\n",
    "true_mu = 5\n",
    "true_sigma = 2\n",
    "N = 100\n",
    "data = np.random.normal(true_mu, true_sigma, N)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Define a function to compute the log likelihood of the data given a mean and standard deviation\n",
    "def log_likelihood(data, mu, sigma):\n",
    "    return np.sum(stats.norm.logpdf(data, mu, sigma))\n",
    "\n",
    "# Make a contour plot of the log likelihood function vs. mu and sigma\n",
    "mu_vals = np.linspace(3, 7, 200)\n",
    "sigma_vals = np.linspace(1, 3, 200)\n",
    "mu_grid, sigma_grid = np.meshgrid(mu_vals, sigma_vals)\n",
    "log_likelihood_grid = np.zeros(mu_grid.shape)\n",
    "for i in range(mu_vals.size):\n",
    "    for j in range(sigma_vals.size):\n",
    "        log_likelihood_grid[j, i] = log_likelihood(data, mu_vals[i], sigma_vals[j])\n",
    "\n",
    "cbar = ax[1].contour(mu_grid, sigma_grid, log_likelihood_grid, 100)\n",
    "# Place a star at the true mu and sigma\n",
    "ax[1].plot(true_mu, true_sigma, 'r*', markersize=15)\n",
    "# Place crosshairs at the maximum likelihood estimate\n",
    "max_likelihood_idx = np.unravel_index(np.argmax(log_likelihood_grid), log_likelihood_grid.shape)\n",
    "ax[1].plot(mu_vals[max_likelihood_idx[1]], sigma_vals[max_likelihood_idx[0]], 'g+', markersize=15)\n",
    "\n",
    "# Add a colorbar and labels\n",
    "fig.colorbar(cbar, ax=ax[1])\n",
    "ax[1].set_xlabel('mu')\n",
    "ax[1].set_ylabel('sigma')\n",
    "ax[1].set_title('Log likelihood of the data')\n",
    "ax[1].legend(['True parameters', 'Maximum likelihood estimate'])\n",
    "\n",
    "# Plot a histogram of the data\n",
    "ax[0].hist(data, bins=20, density=True)\n",
    "ax[0].set_title('Histogram of the data')\n",
    "# add the true density\n",
    "x = np.linspace(0, 10, 100)\n",
    "ax[0].plot(x, stats.norm.pdf(x, true_mu, true_sigma), 'r', linewidth=2)\n",
    "ax[0].legend(['True density', 'Histogram of the data'])\n",
    "# Add the estimated density to the histogram in ax[0]\n",
    "ax[0].plot(x, stats.norm.pdf(x, mu_vals[max_likelihood_idx[1]], sigma_vals[max_likelihood_idx[0]]), 'k--', linewidth=2)\n",
    "ax[0].legend(['True density', 'Estimated density', 'Histogram of the data'])\n",
    "plt.show()\n",
    "\n",
    "# Print the true and estimated parameters\n",
    "print('True mu: ', true_mu)\n",
    "print('True sigma: ', true_sigma)\n",
    "print('Estimated mu (via plotting): ', mu_vals[max_likelihood_idx[1]])\n",
    "print('Estimated sigma: (via plotting)', sigma_vals[max_likelihood_idx[0]])\n",
    "\n",
    "# Print the sample mean and standard deviation\n",
    "print('Sample mean: ', np.mean(data))\n",
    "print('Sample standard deviation: ', np.std(data, ddof=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the MLE through an optimization search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use an optimization algorithm to find the maximum likelihood estimate\n",
    "\n",
    "# Define a function to compute the negative log likelihood for each set of parameters\n",
    "def neg_log_likelihood(params, data):\n",
    "    mu, sigma = params\n",
    "    # Return the minus log likelihood (which we will want to minimize)\n",
    "    return -np.sum(stats.norm.logpdf(data, mu, sigma))\n",
    "\n",
    "# Use the minimize function to find the maximum likelihood estimate\n",
    "result = minimize(neg_log_likelihood, [3, 3], args=(data,))\n",
    "\n",
    "# Now let's compare the results to the previous estimate\n",
    "# Print the true and estimated parameters\n",
    "print('True mu: ', true_mu)\n",
    "print('True sigma: ', true_sigma)\n",
    "print('Estimated mu (via plotting): ', mu_vals[max_likelihood_idx[1]])\n",
    "print('Estimated sigma: (via plotting)', sigma_vals[max_likelihood_idx[0]])\n",
    "print('Estimated mu (via optimization): ', result.x[0])\n",
    "print('Estimated sigma (via optimization): ', result.x[1])\n",
    "\n",
    "# Print the sample mean and standard deviation\n",
    "print('Sample mean: ', np.mean(data))\n",
    "print('Sample standard deviation: ', np.std(data, ddof=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the optimization routine gave us a result that was **very** close to the sample mean and sample variance.  Do you think that was just a coincidence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Calculation of the MLE for Gaussian Distribution\n",
    "To find the maximum likelihood estimators (MLEs) for $\\mu$ and $\\sigma^2$, we maximize the likelihood function with respect to these parameters. Taking the logarithm of the likelihood function, we have:\n",
    "\n",
    "$$\n",
    "\\log L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "To find the MLE for $\\mu$, we differentiate the logarithm of the likelihood function with respect to $\\mu$ and set it equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} \\log L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\mu$, we find:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{x}\n",
    "$$\n",
    "\n",
    "which is the sample mean.\n",
    "\n",
    "To find the MLE for $\\sigma^2$, we differentiate the logarithm of the likelihood function with respect to $\\sigma^2$ and set it equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\sigma^2} \\log L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\sigma^2$, we find:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = s^2\n",
    "$$\n",
    "\n",
    "which is the sample variance.\n",
    "\n",
    "**Therefore, the maximum likelihood estimators (MLEs) for the mean and standard deviation of a Gaussian distribution are the sample mean $\\bar{x}$ and the sample standard deviation $s$, respectively.**  These are very easy to compute as we have seen earlier!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice on your own:\n",
    "1) Generate five different random data sets from five different distributions (normal, beta, log-normal, gamma, t)\n",
    "2) Compute the likelihood of each data set assuming a model with each of the foufiver different distributions.\n",
    "3) Optimize the parameters of for each combination of data and distributions (5 x 5 = 25 total optimizations).\n",
    "4) Make a table to present the MLE log-likelihood versus model for each combination.\n",
    "5) Comment on whether or not you could identify which data set came from which model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
